{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Dataset: Load the mendeley file containing your dataset into a pandas DataFrame or any appropriate data structure in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>denial of normal the con be asked to comment o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just by being able to tweet this insufferable ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that is retarded you too cute to be single tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thought of a real badass mongol style declarat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afro american basho</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  label\n",
       "0  denial of normal the con be asked to comment o...      1\n",
       "1  just by being able to tweet this insufferable ...      1\n",
       "2  that is retarded you too cute to be single tha...      1\n",
       "3  thought of a real badass mongol style declarat...      1\n",
       "4                                afro american basho      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/mendeley/HateSpeechDatasetBalanced.csv\")\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing: Perform any necessary pre-processing steps on the tweets, such as cleaning, tokenization, and normalization. You may also need to handle any missing values or outliers in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write tweets to a text file\n",
    "with open(\"../../preprocessed/mendeley_bert_formatted_data.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for tweet in df[\"content\"]:\n",
    "        file.write(tweet + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Custom BERT Tokenizer: Use the pre-processed dataset to train a custom BERT tokenizer specifically tailored for your hate speech classification task. This tokenizer will convert the tweets into BERT-compatible tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../custom_bert_tokenizer_mendeley\\\\vocab.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# Initialize the tokenizer with desired parameters\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,  # Clean text before tokenization\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=False,\n",
    "    lowercase=False,  # Keep case-sensitive\n",
    "    wordpieces_prefix=\"##\"\n",
    ")\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train(\n",
    "    files=[\"../../preprocessed/mendeley_bert_formatted_data.txt\"],  # Path to formatted data\n",
    "    vocab_size=50000,  # Vocabulary size\n",
    "    min_frequency=2,   # Minimum frequency to include a token in vocabulary\n",
    "    limit_alphabet=1000,  # Limit alphabet characters during training\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]  # Special tokens to include\n",
    ")\n",
    "\n",
    "save_dir = \"../../custom_bert_tokenizer_mendeley\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save the trained tokenizer\n",
    "tokenizer.save_model(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the custom tokenizer to ensure it correctly tokenizes the tweets into BERT-compatible tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input: ['this', 'is', 'a', 'sample', 'bad', 'tweet', 'for', 'testing', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../../custom_bert_tokenizer_mendeley\")\n",
    "\n",
    "# Test the tokenizer on a sample tweet\n",
    "sample_tweet = \"This is a sample bad tweet for testing.\"\n",
    "tokenized_input = tokenizer.tokenize(sample_tweet)\n",
    "print(\"Tokenized input:\", tokenized_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the pre-processed text data (tweets) into tokens using the custom tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../../custom_bert_tokenizer_mendeley\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "df[\"tokenized_text\"] = df[\"content\"].apply(tokenize_text)\n",
    "\n",
    "# Save tokenized data\n",
    "df.to_csv(\"..\\\\..\\\\tokenized_dataset\\\\tokenized_data_mendeley.csv\", index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "al",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
