{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting history \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    # Plotting training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(y_val, y_pred, title):\n",
    "    y_pred_classes = y_pred\n",
    "    y_true_classes = y_val\n",
    "\n",
    "    # Create and plot confusion matrix\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix:  ' + title)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>denial of normal the con be asked to comment o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just by being able to tweet this insufferable ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that is retarded you too cute to be single tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thought of a real badass mongol style declarat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afro american basho</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  label\n",
       "0  denial of normal the con be asked to comment o...      1\n",
       "1  just by being able to tweet this insufferable ...      1\n",
       "2  that is retarded you too cute to be single tha...      1\n",
       "3  thought of a real badass mongol style declarat...      1\n",
       "4                                afro american basho      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/mendeley/HateSpeechDatasetBalanced.csv\")\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>input_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>denial of normal the con be asked to comment o...</td>\n",
       "      <td>1</td>\n",
       "      <td>[denial, of, normal, the, con, be, asked, to, ...</td>\n",
       "      <td>{'input_ids': [tensor(14920), tensor(1997), te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just by being able to tweet this insufferable ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, by, being, able, to, t, ##wee, ##t, thi...</td>\n",
       "      <td>{'input_ids': [tensor(2074), tensor(2011), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that is retarded you too cute to be single tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>[that, is, re, ##tar, ##ded, you, too, cute, t...</td>\n",
       "      <td>{'input_ids': [tensor(2008), tensor(2003), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thought of a real badass mongol style declarat...</td>\n",
       "      <td>1</td>\n",
       "      <td>[thought, of, a, real, bad, ##ass, mongol, sty...</td>\n",
       "      <td>{'input_ids': [tensor(2245), tensor(1997), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afro american basho</td>\n",
       "      <td>1</td>\n",
       "      <td>[afro, american, bash, ##o]</td>\n",
       "      <td>{'input_ids': [tensor(17694), tensor(2137), te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  label  \\\n",
       "0  denial of normal the con be asked to comment o...      1   \n",
       "1  just by being able to tweet this insufferable ...      1   \n",
       "2  that is retarded you too cute to be single tha...      1   \n",
       "3  thought of a real badass mongol style declarat...      1   \n",
       "4                                afro american basho      1   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [denial, of, normal, the, con, be, asked, to, ...   \n",
       "1  [just, by, being, able, to, t, ##wee, ##t, thi...   \n",
       "2  [that, is, re, ##tar, ##ded, you, too, cute, t...   \n",
       "3  [thought, of, a, real, bad, ##ass, mongol, sty...   \n",
       "4                        [afro, american, bash, ##o]   \n",
       "\n",
       "                                      input_features  \n",
       "0  {'input_ids': [tensor(14920), tensor(1997), te...  \n",
       "1  {'input_ids': [tensor(2074), tensor(2011), ten...  \n",
       "2  {'input_ids': [tensor(2008), tensor(2003), ten...  \n",
       "3  {'input_ids': [tensor(2245), tensor(1997), ten...  \n",
       "4  {'input_ids': [tensor(17694), tensor(2137), te...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize and trim content\n",
    "def preprocess_text(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = tokens[:128]  # \n",
    "    return tokens\n",
    "\n",
    "df[\"tokens\"] = df[\"content\"].apply(preprocess_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>input_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>denial of normal the con be asked to comment o...</td>\n",
       "      <td>1</td>\n",
       "      <td>[denial, of, normal, the, con, be, asked, to, ...</td>\n",
       "      <td>{'input_ids': [tensor(14920), tensor(1997), te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just by being able to tweet this insufferable ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, by, being, able, to, t, ##wee, ##t, thi...</td>\n",
       "      <td>{'input_ids': [tensor(2074), tensor(2011), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that is retarded you too cute to be single tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>[that, is, re, ##tar, ##ded, you, too, cute, t...</td>\n",
       "      <td>{'input_ids': [tensor(2008), tensor(2003), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thought of a real badass mongol style declarat...</td>\n",
       "      <td>1</td>\n",
       "      <td>[thought, of, a, real, bad, ##ass, mongol, sty...</td>\n",
       "      <td>{'input_ids': [tensor(2245), tensor(1997), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afro american basho</td>\n",
       "      <td>1</td>\n",
       "      <td>[afro, american, bash, ##o]</td>\n",
       "      <td>{'input_ids': [tensor(17694), tensor(2137), te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  label  \\\n",
       "0  denial of normal the con be asked to comment o...      1   \n",
       "1  just by being able to tweet this insufferable ...      1   \n",
       "2  that is retarded you too cute to be single tha...      1   \n",
       "3  thought of a real badass mongol style declarat...      1   \n",
       "4                                afro american basho      1   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [denial, of, normal, the, con, be, asked, to, ...   \n",
       "1  [just, by, being, able, to, t, ##wee, ##t, thi...   \n",
       "2  [that, is, re, ##tar, ##ded, you, too, cute, t...   \n",
       "3  [thought, of, a, real, bad, ##ass, mongol, sty...   \n",
       "4                        [afro, american, bash, ##o]   \n",
       "\n",
       "                                      input_features  \n",
       "0  {'input_ids': [tensor(14920), tensor(1997), te...  \n",
       "1  {'input_ids': [tensor(2074), tensor(2011), ten...  \n",
       "2  {'input_ids': [tensor(2008), tensor(2003), ten...  \n",
       "3  {'input_ids': [tensor(2245), tensor(1997), ten...  \n",
       "4  {'input_ids': [tensor(17694), tensor(2137), te...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def create_input_features(tokens):\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    segment_ids = [0] * len(input_ids)  #Single sentence, all zeros\n",
    "    padding_length = 128 - len(input_ids)\n",
    "    \n",
    "    # Pad sequences\n",
    "    input_ids += [0] * padding_length\n",
    "    input_mask += [0] * padding_length\n",
    "    segment_ids += [0] * padding_length\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(input_mask),\n",
    "        \"token_type_ids\": torch.tensor(segment_ids)\n",
    "    }\n",
    "\n",
    "df[\"input_features\"] = df[\"tokens\"].apply(create_input_features)\n",
    "\n",
    "# Now df[\"input_features\"] contains the BERT input features for each row\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "labels = torch.tensor(df[\"label\"].values)\n",
    "\n",
    "# Create TensorDataset\n",
    "all_input_ids = torch.stack(df[\"input_features\"].apply(lambda x: x['input_ids']).values.tolist())\n",
    "all_attention_mask = torch.stack(df[\"input_features\"].apply(lambda x: x['attention_mask']).values.tolist())\n",
    "all_token_type_ids = torch.stack(df[\"input_features\"].apply(lambda x: x['token_type_ids']).values.tolist())\n",
    "\n",
    "dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_size = int(0.9 * len(df))\n",
    "val_size = len(df) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save train_dataset\n",
    "with open(\"../../preprocessed/bert_mendeley_train_dataset.pkl\", \"wb\") as train_file:\n",
    "    pickle.dump(train_dataset, train_file)\n",
    "\n",
    "# Save val_dataset\n",
    "with open(\"../../preprocessed/bert_mendeley_val_dataset.pkl\", \"wb\") as val_file:\n",
    "    pickle.dump(val_dataset, val_file)\n",
    "    \n",
    "print(\"Datasets saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load datasets\n",
    "with open(\"../../preprocessed/bert_mendeley_train_dataset.pkl\", \"rb\") as train_file:\n",
    "    train_dataset = pickle.load(train_file)\n",
    "\n",
    "with open(\"../../preprocessed/bert_mendeley_val_dataset.pkl\", \"rb\") as val_file:\n",
    "    val_dataset = pickle.load(val_file)\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "print(\"Datasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prepare bert model\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and validate the model\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "epochs = 3\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Initialize lists to store training history\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    correct_train_preds = 0\n",
    "    total_train_samples = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        token_type_ids = batch[2].to(device)\n",
    "        labels = batch[3].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        correct_train_preds += (preds == labels).sum().item()\n",
    "        total_train_samples += labels.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_accuracy = correct_train_preds / total_train_samples\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_val_preds = 0\n",
    "    total_val_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{epochs}\"):\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            token_type_ids = batch[2].to(device)\n",
    "            labels = batch[3].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1).flatten()\n",
    "            correct_val_preds += (preds == labels).sum().item()\n",
    "            total_val_samples += labels.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = correct_val_preds / total_val_samples\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('../../models/bert_mendeley_model/model')\n",
    "tokenizer.save_pretrained('../../models/bert_mendeley_model/tokenizer')\n",
    "\n",
    "# Save training history\n",
    "history = {\n",
    "    'loss': train_losses,\n",
    "    'val_loss': val_losses,\n",
    "    'accuracy': train_accuracies,\n",
    "    'val_accuracy': val_accuracies\n",
    "}\n",
    "with open(\"../../histories/bert_mendeley_finetuned_history.pkl\", \"wb\") as f:\n",
    "    pickle.dump(history, f)\n",
    "\n",
    "print(\"Training completed and model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Load training history\n",
    "with open(\"training_history.pkl\", \"rb\") as f:\n",
    "    history = pickle.load(f)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history['loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model_path = \"../../models/bert_mendeley_model/model\" \n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Load validation data (assuming you have val_dataset)\n",
    "val_input_ids, val_attention_mask, val_labels = val_dataset.values()\n",
    "\n",
    "# Predictions\n",
    "with torch.no_grad():\n",
    "    logits = model(val_input_ids, attention_mask=val_attention_mask).logits\n",
    "    predicted_labels = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(val_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification report\n",
    "class_report = classification_report(val_labels, predicted_labels)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)  # Replace with your actual function\n",
    "\n",
    "# Plot confusion matrix (use your existing function)\n",
    "plot_confusion_matrix(val_labels, predicted_labels, title=\"Bert finetuned\")  # Replace with your actual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import transformers\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load datasets\n",
    "with open(\"../../preprocessed/bert_mendeley_train_dataset.pkl\", \"rb\") as train_file:\n",
    "    train_dataset = pickle.load(train_file)\n",
    "\n",
    "with open(\"../../preprocessed/bert_mendeley_val_dataset.pkl\", \"rb\") as val_file:\n",
    "    val_dataset = pickle.load(val_file)\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "print(\"Datasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1045,  2572,  1999,  ...,     0,     0,     0],\n",
       "         [22555, 11924, 18059,  ...,     0,     0,     0],\n",
       "         [ 1045,  2001,  2025,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [ 8011,  2080,  2017,  ...,     0,     0,     0],\n",
       "         [ 2515,  2025,  2655,  ...,     0,     0,     0],\n",
       "         [ 1051,  5737,  3844,  ...,     0,     0,     0]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../../models/bert_mendeley_hatespeech_finetuned\",\n",
    "    logging_dir=\"../../models/bert_mendeley_hatespeech_finetuned/logs\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert tensors in tuples to lists\n",
    "input_features = [t[0].tolist() for t in train_dataset]\n",
    "attention_masks = [t[1].tolist() for t in train_dataset]\n",
    "\n",
    "# Create a dictionary with list values\n",
    "data_dict = {'input_features': input_features, 'attention_masks': attention_masks}\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_data = Dataset.from_dict(data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tensors in tuples to lists for validation data\n",
    "val_input_features = [t[0].tolist() for t in val_dataset]\n",
    "val_attention_masks = [t[1].tolist() for t in val_dataset]\n",
    "\n",
    "# Create a dictionary with list values\n",
    "val_data_dict = {'input_features': val_input_features, 'attention_masks': val_attention_masks}\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "val_data = Dataset.from_dict(val_data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bemne\\anaconda3\\envs\\al\\Lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the evaluation metric\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "\n",
    "# Define the evaluation function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompt: save the history of bert as training accuracy, train loss, val accuracy, val loss, plot the accuracy and loss separately, then computer confusion matrix and plot that too\n",
    "\n",
    "# Save the history of bert as training accuracy, train loss, val accuracy, val loss\n",
    "history = trainer.state.history\n",
    "\n",
    "# Plot the accuracy and loss separately\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history[\"train_accuracy\"], label=\"train_accuracy\")\n",
    "plt.plot(history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Compute confusion matrix and plot that too\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = trainer.predict(val_data).predictions.argmax(-1)\n",
    "y_true = val_data[\"labels\"]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.imshow(cm, cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "al",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
