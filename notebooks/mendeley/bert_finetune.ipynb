{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting history \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    # Plotting training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(y_val, y_pred, title):\n",
    "    y_pred_classes = y_pred\n",
    "    y_true_classes = y_val\n",
    "\n",
    "    # Create and plot confusion matrix\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix:  ' + title)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>denial of normal the con be asked to comment o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just by being able to tweet this insufferable ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that is retarded you too cute to be single tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thought of a real badass mongol style declarat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afro american basho</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  label\n",
       "0  denial of normal the con be asked to comment o...      1\n",
       "1  just by being able to tweet this insufferable ...      1\n",
       "2  that is retarded you too cute to be single tha...      1\n",
       "3  thought of a real badass mongol style declarat...      1\n",
       "4                                afro american basho      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/mendeley/HateSpeechDatasetBalanced.csv\")\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>input_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>denial of normal the con be asked to comment o...</td>\n",
       "      <td>1</td>\n",
       "      <td>[denial, of, normal, the, con, be, asked, to, ...</td>\n",
       "      <td>{'input_ids': [tensor(14920), tensor(1997), te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just by being able to tweet this insufferable ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, by, being, able, to, t, ##wee, ##t, thi...</td>\n",
       "      <td>{'input_ids': [tensor(2074), tensor(2011), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that is retarded you too cute to be single tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>[that, is, re, ##tar, ##ded, you, too, cute, t...</td>\n",
       "      <td>{'input_ids': [tensor(2008), tensor(2003), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thought of a real badass mongol style declarat...</td>\n",
       "      <td>1</td>\n",
       "      <td>[thought, of, a, real, bad, ##ass, mongol, sty...</td>\n",
       "      <td>{'input_ids': [tensor(2245), tensor(1997), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afro american basho</td>\n",
       "      <td>1</td>\n",
       "      <td>[afro, american, bash, ##o]</td>\n",
       "      <td>{'input_ids': [tensor(17694), tensor(2137), te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  label  \\\n",
       "0  denial of normal the con be asked to comment o...      1   \n",
       "1  just by being able to tweet this insufferable ...      1   \n",
       "2  that is retarded you too cute to be single tha...      1   \n",
       "3  thought of a real badass mongol style declarat...      1   \n",
       "4                                afro american basho      1   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [denial, of, normal, the, con, be, asked, to, ...   \n",
       "1  [just, by, being, able, to, t, ##wee, ##t, thi...   \n",
       "2  [that, is, re, ##tar, ##ded, you, too, cute, t...   \n",
       "3  [thought, of, a, real, bad, ##ass, mongol, sty...   \n",
       "4                        [afro, american, bash, ##o]   \n",
       "\n",
       "                                      input_features  \n",
       "0  {'input_ids': [tensor(14920), tensor(1997), te...  \n",
       "1  {'input_ids': [tensor(2074), tensor(2011), ten...  \n",
       "2  {'input_ids': [tensor(2008), tensor(2003), ten...  \n",
       "3  {'input_ids': [tensor(2245), tensor(1997), ten...  \n",
       "4  {'input_ids': [tensor(17694), tensor(2137), te...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize and trim content\n",
    "def preprocess_text(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = tokens[:128]  # \n",
    "    return tokens\n",
    "\n",
    "df[\"tokens\"] = df[\"content\"].apply(preprocess_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>input_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>denial of normal the con be asked to comment o...</td>\n",
       "      <td>1</td>\n",
       "      <td>[denial, of, normal, the, con, be, asked, to, ...</td>\n",
       "      <td>{'input_ids': [tensor(14920), tensor(1997), te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just by being able to tweet this insufferable ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, by, being, able, to, t, ##wee, ##t, thi...</td>\n",
       "      <td>{'input_ids': [tensor(2074), tensor(2011), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that is retarded you too cute to be single tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>[that, is, re, ##tar, ##ded, you, too, cute, t...</td>\n",
       "      <td>{'input_ids': [tensor(2008), tensor(2003), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thought of a real badass mongol style declarat...</td>\n",
       "      <td>1</td>\n",
       "      <td>[thought, of, a, real, bad, ##ass, mongol, sty...</td>\n",
       "      <td>{'input_ids': [tensor(2245), tensor(1997), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afro american basho</td>\n",
       "      <td>1</td>\n",
       "      <td>[afro, american, bash, ##o]</td>\n",
       "      <td>{'input_ids': [tensor(17694), tensor(2137), te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  label  \\\n",
       "0  denial of normal the con be asked to comment o...      1   \n",
       "1  just by being able to tweet this insufferable ...      1   \n",
       "2  that is retarded you too cute to be single tha...      1   \n",
       "3  thought of a real badass mongol style declarat...      1   \n",
       "4                                afro american basho      1   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [denial, of, normal, the, con, be, asked, to, ...   \n",
       "1  [just, by, being, able, to, t, ##wee, ##t, thi...   \n",
       "2  [that, is, re, ##tar, ##ded, you, too, cute, t...   \n",
       "3  [thought, of, a, real, bad, ##ass, mongol, sty...   \n",
       "4                        [afro, american, bash, ##o]   \n",
       "\n",
       "                                      input_features  \n",
       "0  {'input_ids': [tensor(14920), tensor(1997), te...  \n",
       "1  {'input_ids': [tensor(2074), tensor(2011), ten...  \n",
       "2  {'input_ids': [tensor(2008), tensor(2003), ten...  \n",
       "3  {'input_ids': [tensor(2245), tensor(1997), ten...  \n",
       "4  {'input_ids': [tensor(17694), tensor(2137), te...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def create_input_features(tokens):\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    segment_ids = [0] * len(input_ids)  #Single sentence, all zeros\n",
    "    padding_length = 128 - len(input_ids)\n",
    "    \n",
    "    # Pad sequences\n",
    "    input_ids += [0] * padding_length\n",
    "    input_mask += [0] * padding_length\n",
    "    segment_ids += [0] * padding_length\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(input_mask),\n",
    "        \"token_type_ids\": torch.tensor(segment_ids)\n",
    "    }\n",
    "\n",
    "df[\"input_features\"] = df[\"tokens\"].apply(create_input_features)\n",
    "\n",
    "# Now df[\"input_features\"] contains the BERT input features for each row\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "labels = torch.tensor(df[\"label\"].values)\n",
    "\n",
    "# Create TensorDataset\n",
    "all_input_ids = torch.stack(df[\"input_features\"].apply(lambda x: x['input_ids']).values.tolist())\n",
    "all_attention_mask = torch.stack(df[\"input_features\"].apply(lambda x: x['attention_mask']).values.tolist())\n",
    "all_token_type_ids = torch.stack(df[\"input_features\"].apply(lambda x: x['token_type_ids']).values.tolist())\n",
    "\n",
    "dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_size = int(0.9 * len(df))\n",
    "val_size = len(df) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save train_dataset\n",
    "with open(\"../../preprocessed/bert_mendeley_train_dataset.pkl\", \"wb\") as train_file:\n",
    "    pickle.dump(train_dataset, train_file)\n",
    "\n",
    "# Save val_dataset\n",
    "with open(\"../../preprocessed/bert_mendeley_val_dataset.pkl\", \"wb\") as val_file:\n",
    "    pickle.dump(val_dataset, val_file)\n",
    "    \n",
    "print(\"Datasets saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load datasets\n",
    "with open(\"../../preprocessed/bert_mendeley_train_dataset.pkl\", \"rb\") as train_file:\n",
    "    train_dataset = pickle.load(train_file)\n",
    "\n",
    "with open(\"../../preprocessed/bert_mendeley_val_dataset.pkl\", \"rb\") as val_file:\n",
    "    val_dataset = pickle.load(val_file)\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "print(\"Datasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prepare bert model\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\bemne\\anaconda3\\envs\\al\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Train and validate the model\n",
    "\n",
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "epochs = 3\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Initialize lists to store training history\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    correct_train_preds = 0\n",
    "    total_train_samples = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        token_type_ids = batch[2].to(device)\n",
    "        labels = batch[3].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        correct_train_preds += (preds == labels).sum().item()\n",
    "        total_train_samples += labels.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_accuracy = correct_train_preds / total_train_samples\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_val_preds = 0\n",
    "    total_val_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            token_type_ids = batch[2].to(device)\n",
    "            labels = batch[3].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1).flatten()\n",
    "            correct_val_preds += (preds == labels).sum().item()\n",
    "            total_val_samples += labels.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = correct_val_preds / total_val_samples\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('../../models/bert_mendeley_model/model')\n",
    "tokenizer.save_pretrained('../../models/bert_mendeley_model/tokenizer')\n",
    "\n",
    "# Save training history\n",
    "history = {\n",
    "    'loss': train_losses,\n",
    "    'val_loss': val_losses,\n",
    "    'accuracy': train_accuracies,\n",
    "    'val_accuracy': val_accuracies\n",
    "}\n",
    "with open(\"../../histories/bert_mendeley_finetuned_history.pkl\", \"wb\") as f:\n",
    "    pickle.dump(history, f)\n",
    "\n",
    "print(\"Training completed and model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Load training history\n",
    "with open(\"training_history.pkl\", \"rb\") as f:\n",
    "    history = pickle.load(f)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history['train_losses'], label='Train Loss')\n",
    "plt.plot(history['val_losses'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history['train_accuracies'], label='Train Accuracy')\n",
    "plt.plot(history['val_accuracies'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model_path = \"../../models/bert_mendeley_model/model\" \n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Load validation data (assuming you have val_dataset)\n",
    "val_input_ids, val_attention_mask, val_labels = val_dataset.values()\n",
    "\n",
    "# Predictions\n",
    "with torch.no_grad():\n",
    "    logits = model(val_input_ids, attention_mask=val_attention_mask).logits\n",
    "    predicted_labels = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(val_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification report\n",
    "class_report = classification_report(val_labels, predicted_labels)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)  # Replace with your actual function\n",
    "\n",
    "# Plot confusion matrix (use your existing function)\n",
    "plot_confusion_matrix(val_labels, predicted_labels, title=\"Bert finetuned\")  # Replace with your actual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "al",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
